{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"yv3jaWFQu9qK","executionInfo":{"status":"ok","timestamp":1644921781306,"user_tz":0,"elapsed":6165,"user":{"displayName":"Erdogan Kervanli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08310837901031036577"}}},"outputs":[],"source":["import re\n","import numpy as np\n","import scipy\n","import itertools\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from scipy.spatial.distance import cdist\n","from collections import Counter\n","from random import choice\n","import pandas as pd\n","\n","test_data = pd.read_csv('/content/drive/MyDrive/poster_presentation/dataset/test.csv')\n","train_data = pd.read_csv('/content/drive/MyDrive/poster_presentation/dataset/train.csv')"]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"526pL2AZPRIB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644921781307,"user_tz":0,"elapsed":16,"user":{"displayName":"Erdogan Kervanli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08310837901031036577"}},"outputId":"8e1a130a-90db-41ac-da63-014eca0962ba"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 13.6 gigabytes of available RAM\n","\n","Not using a high-RAM runtime\n"]}]},{"cell_type":"code","source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"metadata":{"id":"3EOEBd3qPVD5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644921784250,"user_tz":0,"elapsed":221,"user":{"displayName":"Erdogan Kervanli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08310837901031036577"}},"outputId":"5ed49b24-2e31-44ab-f034-8b85bb616d23"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","import timeit\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  print(\n","      '\\n\\nThis error most likely means that this notebook is not '\n","      'configured to use a GPU.  Change this in Notebook Settings via the '\n","      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n","  raise SystemError('GPU device not found')\n","\n","def cpu():\n","  with tf.device('/cpu:0'):\n","    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n","    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n","    return tf.math.reduce_sum(net_cpu)\n","\n","def gpu():\n","  with tf.device('/device:GPU:0'):\n","    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n","    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n","    return tf.math.reduce_sum(net_gpu)\n","  \n","# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n","cpu()\n","gpu()\n","\n","# Run the op several times.\n","print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n","      '(batch x height x width x channel). Sum of ten runs.')\n","print('CPU (s):')\n","cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n","print(cpu_time)\n","print('GPU (s):')\n","gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n","print(gpu_time)\n","print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66QiBY6251K5","executionInfo":{"status":"ok","timestamp":1644921795805,"user_tz":0,"elapsed":11558,"user":{"displayName":"Erdogan Kervanli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08310837901031036577"}},"outputId":"b37bc616-49b2-423d-cecf-fef1a10c5eaa"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n","CPU (s):\n","2.9673076980000133\n","GPU (s):\n","0.03925987599996006\n","GPU speedup over CPU: 75x\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bs_6TCK_16UH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"14621d0a-0ce9-4abe-83bb-ec8f55dfeba7"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: FutureWarning: The default value of regex will change from True to False in a future version.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: FutureWarning: The default value of regex will change from True to False in a future version.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: FutureWarning: The default value of regex will change from True to False in a future version.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_4 (InputLayer)           [(None, 1349)]       0           []                               \n","                                                                                                  \n"," embedding_3 (Embedding)        (None, 1349, 30)     5905140     ['input_4[0][0]']                \n","                                                                                                  \n"," lstm_3 (LSTM)                  (None, 128)          81408       ['embedding_3[0][0]']            \n","                                                                                                  \n"," dense_3 (Dense)                (None, 64)           8256        ['lstm_3[0][0]']                 \n","                                                                                                  \n"," Malignant (Dense)              (None, 1)            65          ['dense_3[0][0]']                \n","                                                                                                  \n"," Highly_Malignant (Dense)       (None, 1)            65          ['dense_3[0][0]']                \n","                                                                                                  \n"," Rude (Dense)                   (None, 1)            65          ['dense_3[0][0]']                \n","                                                                                                  \n"," Threat (Dense)                 (None, 1)            65          ['dense_3[0][0]']                \n","                                                                                                  \n"," Abuse (Dense)                  (None, 1)            65          ['dense_3[0][0]']                \n","                                                                                                  \n"," Loathe (Dense)                 (None, 1)            65          ['dense_3[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 5,995,194\n","Trainable params: 5,995,194\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/100\n"," 72751/159571 [============>.................] - ETA: 1:32:29 - loss: 0.0000e+00 - Malignant_loss: 0.0000e+00 - Highly_Malignant_loss: 0.0000e+00 - Rude_loss: 0.0000e+00 - Threat_loss: 0.0000e+00 - Abuse_loss: 0.0000e+00 - Loathe_loss: 0.0000e+00 - Malignant_accuracy: 0.9053 - Malignant_precision_3: 0.0000e+00 - Malignant_recall_3: 0.0000e+00 - Highly_Malignant_accuracy: 0.9902 - Highly_Malignant_precision_3: 0.0000e+00 - Highly_Malignant_recall_3: 0.0000e+00 - Rude_accuracy: 0.9481 - Rude_precision_3: 0.0000e+00 - Rude_recall_3: 0.0000e+00 - Threat_accuracy: 0.9969 - Threat_precision_3: 0.0000e+00 - Threat_recall_3: 0.0000e+00 - Abuse_accuracy: 0.9510 - Abuse_precision_3: 0.0333 - Abuse_recall_3: 2.8289e-04 - Loathe_accuracy: 0.9912 - Loathe_precision_3: 0.0000e+00 - Loathe_recall_3: 0.0000e+00"]}],"source":["#preprocess\n","import tensorflow as tf\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import  stopwords\n","import string\n","import re\n","    \n","train_data['length'] = train_data['comment_text'].str.len()\n","train_data.head(10)\n","\n","# Convert all messages to lower case\n","train_data['comment_text'] = train_data['comment_text'].str.lower()\n","\n","# Replace email addresses with 'email'\n","train_data['comment_text'] = train_data['comment_text'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n","                                 'emailaddress')\n","\n","# Replace URLs with 'webaddress'\n","train_data['comment_text'] = train_data['comment_text'].str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n","                                  'webaddress')\n","\n","# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n","train_data['comment_text'] = train_data['comment_text'].str.replace(r'£|\\$', 'dollars')\n","    \n","# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n","train_data['comment_text'] = train_data['comment_text'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n","                                  'phonenumber')\n","\n","\n","# Replace numbers with 'numbr'\n","train_data['comment_text'] = train_data['comment_text'].str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n","\n","\n","train_data['comment_text'] = train_data['comment_text'].apply(lambda x: ' '.join(\n","    term for term in x.split() if term not in string.punctuation))\n","\n","stop_words = set(stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure'])\n","train_data['comment_text'] = train_data['comment_text'].apply(lambda x: ' '.join(\n","    term for term in x.split() if term not in stop_words))\n","\n","lem=WordNetLemmatizer()\n","train_data['comment_text'] = train_data['comment_text'].apply(lambda x: ' '.join(\n"," lem.lemmatize(t) for t in x.split()))\n","\n","train_data['clean_length'] = train_data.comment_text.str.len()\n","\n","one_hot_labels=np.array(train_data.iloc[:,2:8])\n","one_hot_labels=np.array(train_data.iloc[:,2:8])\n","class1=np.array(train_data.iloc[:,2])\n","class2=np.array(train_data.iloc[:,3])\n","class3=np.array(train_data.iloc[:,4])\n","class4=np.array(train_data.iloc[:,5])\n","class5=np.array(train_data.iloc[:,6])\n","class6=np.array(train_data.iloc[:,7])\n","\n","lang_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","lang_tokenizer.fit_on_texts(train_data['comment_text'])\n","\n","## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n","## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n","tensor = lang_tokenizer.texts_to_sequences(train_data['comment_text']) \n","tensor=tf.keras.preprocessing.sequence.pad_sequences(tensor)\n","\n","vocab_size=len(lang_tokenizer.word_counts.keys())\n","max_len=tensor.shape[-1]\n","num_classes=6\n","\n","#data processing - create list of comment_texts\n","train_split = []\n","for row in train_data['comment_text']:\n","    train_split.append(row)\n","\n","\n","from numpy import array\n","import tensorflow\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten , Input\n","from tensorflow.keras.layers import Embedding,LSTM\n","from tensorflow.keras.models import Model\n","\n","\n","# define the model\n","model = Sequential()\n","\n","\n","input_shape=(max_len,)\n","\n","inp=Input(input_shape)\n","emb=Embedding(vocab_size, 30)(inp)\n","lstm=LSTM(128,return_sequences=False)(emb)\n","dense=Dense(64)(lstm)\n","output_layers=[]\n","\n","#for i in range (0,num_classes):\n","output_layers.append(Dense(1, activation='sigmoid',name='Malignant')(dense))\n","output_layers.append(Dense(1, activation='sigmoid',name='Highly_Malignant')(dense))\n","output_layers.append(Dense(1, activation='sigmoid',name='Rude')(dense))\n","output_layers.append(Dense(1, activation='sigmoid',name='Threat')(dense))\n","output_layers.append(Dense(1, activation='sigmoid',name='Abuse')(dense))\n","output_layers.append(Dense(1, activation='sigmoid',name='Loathe')(dense))\n","\n","adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n","\n","model = Model (inputs=inp,outputs=output_layers)\n","model.compile(optimizer=adam, loss='categorical_crossentropy',\n","            metrics=['accuracy', tf.keras.metrics.Precision(), tensorflow.keras.metrics.Recall()])\n","\n","# summarize the model\n","print(model.summary())\n","# fit the model\n","model.fit(tensor, [class1, class2, class3, class4, class5, class6], epochs=100,batch_size=1 ,verbose=1,validation_steps=None)\n","# evaluate the model\n","#loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n","#print('Accuracy: %f' % (accuracy*100))"]},{"cell_type":"code","source":[""],"metadata":{"id":"H4mCynFL5upc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New section"],"metadata":{"id":"bkCuh_Kqvg0I"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"OvxGgssYp301"}},{"cell_type":"code","source":[""],"metadata":{"id":"5WN6buO4p4Q8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NT1_OeB7EyKk"},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsxaiWzGtkmi"},"outputs":[],"source":["model.save('IP40_model1.hdf5')"]},{"cell_type":"markdown","metadata":{"id":"DIeLXhZaQsXI"},"source":["# New section"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"LSTM.ipynb","provenance":[],"machine_shape":"hm","mount_file_id":"1V1lLY7QqbQCTLNb1Jc7KmbZkVWHAJDCR","authorship_tag":"ABX9TyM4Y2CPc3l+LF4aFmfikeAK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}